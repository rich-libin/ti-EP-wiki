<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">

<!-- Mirrored from processors.wiki.ti.com/index.php/MCSDK_HPC_3.x_OpenMPI by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 01 Dec 2020 09:52:41 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="UTF-8"/>
<title>MCSDK HPC 3.x OpenMPI - Texas Instruments Wiki</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"MCSDK_HPC_3.x_OpenMPI","wgTitle":"MCSDK HPC 3.x OpenMPI","wgCurRevisionId":206745,"wgRevisionId":206745,"wgArticleId":33143,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Pages with broken file links"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"MCSDK_HPC_3.x_OpenMPI","wgRelevantArticleId":33143,"wgRequestId":"cc33cc99768759c4a6455311","wgIsProbablyEditable":false,"wgRelevantPageIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[]});mw.loader.state({"site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"ready","user.tokens":"loading","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@19o3a1s",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});mw.loader.load(["site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","skins.vector.js"]);});</script>
<link rel="stylesheet" href="https://processors.wiki.ti.com/load.php?debug=false&amp;lang=en&amp;modules=mediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles&amp;only=styles&amp;skin=vector"/>
<script async="" src="https://processors.wiki.ti.com/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="https://processors.wiki.ti.com/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.31.6"/>
<link rel="shortcut icon" href="https://processors.wiki.ti.com/favicon.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="https://processors.wiki.ti.com/opensearch_desc.php" title="Texas Instruments Wiki (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="https://processors.wiki.ti.com/api.php?action=rsd"/>
<link rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="alternate" type="application/atom+xml" title="Texas Instruments Wiki Atom feed" href="https://processors.wiki.ti.com/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<!--[if lt IE 9]><script src="/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=Vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-MCSDK_HPC_3_x_OpenMPI rootpage-MCSDK_HPC_3_x_OpenMPI skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><div id="localNotice" lang="en" dir="ltr"><div class="mw-parser-output"><p><br />
<span style="color:#ff0000"><b>NOTICE: The Processors Wiki will End-of-Life on January 15, 2021. It is recommended to download any files or other content you may need that are hosted on processors.wiki.ti.com. The site is now set to read only.</b></span>
</p></div></div></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">MCSDK HPC 3.x OpenMPI</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Texas Instruments Wiki</div>				<div id="contentSub"></div>
								<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p><a href="File_TIBanner.html" class="image"><img alt="TIBanner.png" src="https://processors.wiki.ti.com/images/c/cd/TIBanner.png" width="667" height="87" /></a>
</p><p><br /><font size="5"><b>Open MPI Runtime</b><br /></font><br /><font size="5"><b>Version 1.0.0.21</b></font><br /><br /><font size="5"><b>User Guide</b></font><br /><br />
</p><p><i>Last updated: 09/18/2015</i>
</p>
<hr />
<div id="toc" class="toc"><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Documentation_.26_Tutorials"><span class="tocnumber">2</span> <span class="toctext">Documentation &amp; Tutorials</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#TI.27s_Open_MPI_enhancements"><span class="tocnumber">3</span> <span class="toctext">TI's Open MPI enhancements</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Cluster_definition_constraints"><span class="tocnumber">3.1</span> <span class="toctext">Cluster definition constraints</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#HPC_programming_paradigms"><span class="tocnumber">3.2</span> <span class="toctext">HPC programming paradigms</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Open_MPI_Run_time_Parameters"><span class="tocnumber">4</span> <span class="toctext">Open MPI Run time Parameters</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#MCA_Parameters"><span class="tocnumber">4.1</span> <span class="toctext">MCA Parameters</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#MCA_Parameters_Example_and_Usage"><span class="tocnumber">4.2</span> <span class="toctext">MCA Parameters Example and Usage</span></a>
<ul>
<li class="toclevel-3 tocsection-9"><a href="#Get_the_MCA_information"><span class="tocnumber">4.2.1</span> <span class="toctext">Get the MCA information</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="#MCA_Usage"><span class="tocnumber">4.2.2</span> <span class="toctext">MCA Usage</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Some_useful_TI-Open_MPI_specific_MCA_parameters"><span class="tocnumber">4.2.3</span> <span class="toctext">Some useful TI-Open MPI specific MCA parameters</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#An_MPI_Example"><span class="tocnumber">5</span> <span class="toctext">An MPI Example</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="#Building_the_testmpi_example"><span class="tocnumber">5.1</span> <span class="toctext">Building the testmpi example</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Running_the_testmpi_example"><span class="tocnumber">5.2</span> <span class="toctext">Running the testmpi example</span></a>
<ul>
<li class="toclevel-3 tocsection-15"><a href="#Selecting_the_BTL"><span class="tocnumber">5.2.1</span> <span class="toctext">Selecting the BTL</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-16"><a href="#Code_Walkthrough_of_the_testmpi_example"><span class="tocnumber">5.3</span> <span class="toctext">Code Walkthrough of the testmpi example</span></a>
<ul>
<li class="toclevel-3 tocsection-17"><a href="#Building_testmpi"><span class="tocnumber">5.3.1</span> <span class="toctext">Building testmpi</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Source_Code_Analysis"><span class="tocnumber">5.3.2</span> <span class="toctext">Source Code Analysis</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-19"><a href="#Demos_included_in_MCSDK-HPC_distribution"><span class="tocnumber">6</span> <span class="toctext">Demos included in MCSDK-HPC distribution</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#Troubleshooting_Guidelines"><span class="tocnumber">8</span> <span class="toctext">Troubleshooting Guidelines</span></a>
<ul>
<li class="toclevel-2 tocsection-22"><a href="#Dependencies"><span class="tocnumber">8.1</span> <span class="toctext">Dependencies</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#FAQ"><span class="tocnumber">9</span> <span class="toctext">FAQ</span></a></li>
</ul>
</div>

<h1><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>Open MPI is an open source, high-performance implementation of MPI (Message Passing Interface) which is a standardized API used for parallel and/or distributed computing. The current release is based on Open MPI 1.7.1 (www.open-mpi.org). MPI program allows concurrent operation of multiple instances of identical program on all nodes within "MPI Communication World". Instances of same program can communicate with each other using Message Passing Interface APIs.
</p><p><br />
<a href="File_All_MPI_Intro.html" class="image" title="MPI_Intro.png"><img alt="MPI_Intro.png" src="https://processors.wiki.ti.com/images/8/85/All_MPI_Intro.png" width="617" height="311" /></a>
<br />
</p>
<h1><span id="Documentation_&amp;_Tutorials"></span><span class="mw-headline" id="Documentation_.26_Tutorials">Documentation &amp; Tutorials</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=2" title="Edit section: Documentation &amp; Tutorials">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>Good documentation on Open MPI could be found at this website <a rel="nofollow" class="external autonumber" href="http://www.open-mpi.org/doc/">[1]</a>
Also, video tutorials for Open MPI could be found here <a rel="nofollow" class="external autonumber" href="http://www.open-mpi.org/video/">[2]</a>.
A lot of information is also present at <a rel="nofollow" class="external autonumber" href="http://www.open-mpi.org/faq/">[3]</a>
</p>
<h1><span id="TI's_Open_MPI_enhancements"></span><span class="mw-headline" id="TI.27s_Open_MPI_enhancements">TI's Open MPI enhancements</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=3" title="Edit section: TI&#039;s Open MPI enhancements">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>The TI version of Open MPI extends the Open MPI by supporting two additional hardware transport layers- SRIO and Hyperlink. This was achieved by enhancing the BTL (Byte Transport Layer) of the Open MPI with the support for SRIO and Hyperlink.
<br />
<a href="File_TI_Additions_Intro.html" class="image" title="TI_Additions_Intro.png"><img alt="TI_Additions_Intro.png" src="https://processors.wiki.ti.com/images/4/48/TI_Additions_Intro.png" width="683" height="353" /></a>
<br />
</p><p>More details on TI's Open MPI over <b>SRIO</b> can be found here <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-15.html">[4]</a>
</p><p>More Details on TI's Open MPI over <b>Hyperlink</b> can be found here  <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-16.html">[5]</a>
</p>
<h2><span class="mw-headline" id="Cluster_definition_constraints">Cluster definition constraints</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=4" title="Edit section: Cluster definition constraints">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>We specify cluster as collection of multiple K2H nodes, but certain constraints apply. As a rule of thumb it is highly advisable to populate complete cartridge (i.e. <b>use all 4 nodes in cartridge</b>) and also to select neighboring cartridges with many interconnect links.
</p>
<ul><li>TCP: Any combination of nodes can be used since each K2H node is connected to the router (star topology). But in case HLINK or SRIO connections are used, certain constraints apply:</li>
<li>HLINK: clusters with two or four nodes are allowed. If three nodes cluster is used, TCP BTL need to be specified to maintain link between edge nodes (those that are not immediate neighbors). If bigger cluster is used, pleas make sure that fully populated cartridges (all four nodes in a cartridge) are specified. In that case HLINK is always selected as it is highest performing transport interface.</li>
<li>SRIO: disjoint nodes in a cluster are not allowed. There has to be a physical SRIO connection between at least two nodes in the cluster. SRIO traffic is not routed via nodes that are not in the cluster. Please refer to the below picture for 2D-torus SRIO interconnect topology:</li></ul>
<p><a href="File_Moonshot-2D-torus-topology.html" class="image"><img alt="Moonshot-2D-torus-topology.png" src="https://processors.wiki.ti.com/images/a/ac/Moonshot-2D-torus-topology.png" width="640" height="472" /></a><br />
Examples of good SRIO/HLINK cluster: 
</p>
<ul><li>c1n1 c1n2 c1n3 c1n4 c2n1 c2n2 c2n3 c2n4</li>
<li>c1n1 c1n2 c1n3 c1n4 c4n1 c4n2 c4n3 c4n4</li></ul>
<p>Example of bad SRIO/HLINK cluster: 
</p>
<ul><li>c1n1 c1n2 c1n3 c1n4 c15n1 c15n2 c15n3 c15n4 (c1n nodes and c15n nodes cannot communicate over SRIO or Hyperlink)</li></ul>
<p><br />
</p>
<h2><span class="mw-headline" id="HPC_programming_paradigms">HPC programming paradigms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=5" title="Edit section: HPC programming paradigms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Here is an overview of how Open MPI works across multiple TI keystone devices
</p><p><br />
<a href="https://processors.wiki.ti.com/index.php?title=Special:Upload&amp;wpDestFile=Open_MPI_TI_Implementation_details.png" class="new" title="File:Open MPI TI Implementation details.png">Open MPI_TI_Implementation details.png</a>
</p><p><br />
</p>
<h1><span class="mw-headline" id="Open_MPI_Run_time_Parameters">Open MPI Run time Parameters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=6" title="Edit section: Open MPI Run time Parameters">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>This section details the Open MPI's run time environment and its parameters which could be useful for configuration and debugging.
</p>
<h2><span class="mw-headline" id="MCA_Parameters">MCA Parameters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=7" title="Edit section: MCA Parameters">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>MCA parameters are the basic unit of run-time tuning for Open MPI. Access to these parameters allows users to change internal Open MPI parameter values at run time. If a task can be implemented in multiple, user-discernible ways, it is optimal to implement as many as possible and make choosing between them be an MCA parameter.This is a service provided by the MCA base. It does not mean that they are restricted to the MCA components of frameworks. OPAL, ORTE, and OMPI projects all have “base” parameters. The MCA base allows users to be proactive and tweak Open MPI's behavior for their environment. It also allows users to experiment with the parameter space to find the best configuration for their specific system.
<br />
</p>
<h2><span class="mw-headline" id="MCA_Parameters_Example_and_Usage">MCA Parameters Example and Usage</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=8" title="Edit section: MCA Parameters Example and Usage">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Here are two examples of MCA parameters usage
</p>
<h3><span class="mw-headline" id="Get_the_MCA_information">Get the MCA information</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=9" title="Edit section: Get the MCA information">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <b>ompi_info</b> command can list the parameters for a given component, all the parameters for a specific framework, or all parameters.
</p><p>Show all the MCA parameters for all components that ompi_info finds
</p>
<pre>/opt/ti-Open MPI/bin/ompi_info --all
</pre>
<p>Show all the MCA parameters for all BTL components
</p>
<pre>/opt/ti-Open MPI/bin/ompi_info –param btl all
</pre>
<p>Show all the MCA parameters for TCP BTL component
</p>
<pre>/opt/ti-Open MPI/bin/ompi_info –param btl tcp
</pre>
<h3><span class="mw-headline" id="MCA_Usage">MCA Usage</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=10" title="Edit section: MCA Usage">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <b>mpirun</b> command executes serial and parallel jobs in Open MPI
</p>
<pre>/opt/ti-Open MPI/bin/mpirun –mca orte_base_help_aggregate 0 –mca btl_base_verbose 100 –mca btl self, tcp –np 2 –host k2node1, k2node2 /home/mpiuser/nbody 1000
</pre>
<p>where /home/mpiuser/nbody is the job which will be run parallel on two nodes k2node1 and k2node2.
</p>
<h3><span class="mw-headline" id="Some_useful_TI-Open_MPI_specific_MCA_parameters">Some useful TI-Open MPI specific MCA parameters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=11" title="Edit section: Some useful TI-Open MPI specific MCA parameters">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li>Select BTL subset:</li></ul>
<pre> --mca btl self,srio,hlink ... put list of BTLs you want to use
</pre>
<ul><li>Disable Hyperlink RX thread:</li></ul>
<pre> --mca btl_hlink_rx_thread 0 ... by default it is enabled (1), which allows larger non-blocking message sizes.
 But for non-blocking messages up to 2MB, and for 10-15% improved performance it is better to disable this thread. It significantly (20x) improves BW over diagonal links.
</pre>
<ul><li>Increase SRIO PDSP CREDIT period:</li></ul>
<pre> --mca btl_srio_pdsp_credit_period 4 ... by default it is set to 1, which means one credit packet is sent after each data message. 
 By increasing this period (2..4) SRIO BW is increased for multi-hop paths (allows message pipelining).
</pre>
<p><br />
</p>
<h1><span class="mw-headline" id="An_MPI_Example">An MPI Example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=12" title="Edit section: An MPI Example">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>One of the simplest Open MPI demo which runs on a cluster of A15s is testmpi. This example collects and distributes hostnames of nodes in MPI Communication World (group of nodes executing common program).
</p>
<h2><span class="mw-headline" id="Building_the_testmpi_example">Building the testmpi example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=13" title="Edit section: Building the testmpi example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Source code of testmpi can be found by installing ti-Open MPI source .
</p>
<pre> apt-get source ti-Open MPI
</pre>
<p>The testmpi source file (testmpi.c) and makefile will be present at ti-Open MPI-1.0.0.21/ti-examples/testmpi
</p><p>Now go to the directory and build the testmpi application
</p>
<pre> cd ti-Open MPI-1.0.0.21/ti-examples/testmpi
 make
</pre>
<p>Make sure that the testmpi application is built and present at the same location in all the participating nodes before attempting to run.
</p><p><br />
</p>
<h2><span class="mw-headline" id="Running_the_testmpi_example">Running the testmpi example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=14" title="Edit section: Running the testmpi example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The testmpi application uses MPI APIs to initialize and set up MPI, print out Hello World message from A15 cluster, and finalize MPI. The testmpi demo is executed using standard MPI runtime commands. MPI offers many command line arguments to tune run-time behavior, like selection of verbosity or selection of transport interfaces.
</p>
<pre>     ~/ti-Open MPI-1.0.0.21/ti-examples/testmpi# /opt/ti-Open MPI/bin/mpirun --mca orte_base_help_aggregate 0  --mca btl self,tcp -np 2 -host k2hnode1,k2hnode1./testmpi
</pre>
<p>The command above runs the testmpi application over two K2H nodes, k2hnode1 and k2hnode2.
</p><p>The output of the same would look like this
</p>
<pre>Hello world from processor k2hnode1, rank 1 out of 2 processors
locally obtained hostname k2hnode1
Hello world from processor k2hnode2, rank 0 out of 2 processors
locally obtained hostname k2hnode2
</pre>
<p><br />
</p>
<h3><span class="mw-headline" id="Selecting_the_BTL">Selecting the BTL</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=15" title="Edit section: Selecting the BTL">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Majority of MPI framework is agnostic of transport interface details. BTL modules (Byte Transfer Layer) provide implementation for various transport interfaces. Argument "--mca btl XXX,YYY,ZZZ" defines list of transport interfaces that will be used for data exchange during execution. Current implementation offers 5 (BTL) transport interfaces: loopback (self), shared memory (sm), TCP/IP (tcp), Serial RapidIO (srio) and Hyperlink (hlink). <br />
In the above example, only self and TCP BTLs are used. MPI programming paradigm assumes that any node in communication world can communicate to another node in communication world. This is obvious for "star-like" topologies, e.g. nodes connected to the switch over Ethernet, or processes running on same node using shared memory transport interface. In general, unconstrained communication capability requires external switching resource. ti-Open MPI solution (based on Open MPI 1.7.1) provides two new BTLs: srio and hlink for K2H specific transport interfaces.<br />
</p>
<ul><li>BTL Serial RapidIO (srio) allows MPI communication between nodes using on-chip (Navigator/PDSP) router <a rel="nofollow" class="external autonumber" href="http://www.ti.com/lit/ug/sprugr9h/sprugr9h.pdf">[6]</a>. More MPI over SRIO details provided here <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-15.html">[7]</a><br /></li>
<li>BTL Hyperlink allows MPI communication between nodes over Hyperlink interface. More details provided here <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-16.html">[8]</a>.</li></ul>
<p>Transport interfaces are always selected (if physical connection exists) in following way: 
</p>
<ul><li>HLINK preferred over SRIO and TCP</li>
<li>SRIO or HLINK preffered over TCP.</li></ul>
<p>In some (rare) cases, if cluster includes traffic with high number of hops, TCP might be preffered over SRIO.
Number of hops and SRIO routing can be checked using routingTableGenTest utility (installed with mcsdk-hpc). Details are provided in <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-15.html#Useful_SRIO_utilities">[9]</a><br />
Only one transport interface is selected for one pair of nodes, but one node can use all 3 interfaces for various nodes.<br />
If SRIO or HLINK BTL is used, it is allowed to have only one MPI rank per K2H node.<br />
But if TCP and SM are used, it is possible to have more than one rank per K2H node.<br />
</p><p><b>Note</b>: Launching and initial interfacing (e.g. exchange of TCP ports) of all instances is handled by ORTED (Open MPI specific) process started typically using "SSH". Properly configured "SSH" is necessary (TCP/IP connectivity is needed independent of other available transport interfaces). Please note that IP and hostname (of same SoC) are treated as separate SSH entries (with keys typically copied using <i>ssh-copy-id</i>).
</p><p><br />
</p>
<h2><span class="mw-headline" id="Code_Walkthrough_of_the_testmpi_example">Code Walkthrough of the testmpi example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=16" title="Edit section: Code Walkthrough of the testmpi example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Here are some details regarding the MPI example code and its building details
</p>
<h4><span class="mw-headline" id="Building_testmpi">Building testmpi</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=17" title="Edit section: Building testmpi">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A closer look at the Makefile ( present at ti-Open MPI-1.0.0.21/ti-examples/testmpi) reveals that it exports the environment variables and defines the rule for building, installing, and cleaning the testmpi executable.
</p>
<pre>include ../../make.inc
export OPAL_PREFIX=${TARGET_ROOTDIR}/opt/ti-Open MPI
export PATH:=${TARGET_ROOTDIR}/opt/ti-Open MPI/bin:$(PATH)
export LD_LIBRARY_PATH=${TARGET_ROOTDIR}/opt/ti-Open MPI/lib:${TARGET_ROOTDIR}/lib
export C_INCLUDE_PATH=${TARGET_ROOTDIR}/opt/ti-Open MPI/include
EXECS=testmpi
MPICC=${TARGET_ROOTDIR}/opt/ti-Open MPI/bin/mpicc
all: ${EXECS}
testmpi: testmpi.c
      ${MPICC} -o testmpi testmpi.c
clean:
     rm ${EXECS}
$(TARGET_ROOTDIR)$(INSTALL_DIR)/testmpi:
     mkdir -p $(TARGET_ROOTDIR)$(INSTALL_DIR)/testmpi
install: $(TARGET_ROOTDIR)$(INSTALL_DIR)/testmpi
     cp ${EXECS} README $(TARGET_ROOTDIR)$(INSTALL_DIR)/testmpi
</pre>
<p><br />
The mpicc is a wrapper compiler built on top of gcc, for compiling MPI programs.
</p>
<h4><span class="mw-headline" id="Source_Code_Analysis">Source Code Analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=18" title="Edit section: Source Code Analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The source code for this testmpi example is testmpi.c. A closer look would reveal the below
</p>
<pre> MPI_Init (&amp;argc, &amp;argv);  /* Startup */
 /* starts MPI */
 MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank);  /* Who am I?*/
 /* get current process id */
 MPI_Comm_size (MPI_COMM_WORLD, &amp;size);/* How many peers do I have */
 /* get number of processes */
 {
    /* Get the name of the processor */
   char processor_name[320];
   int name_len;
   MPI_Get_processor_name(processor_name, &amp;name_len);
   printf("Hello world from processor&#160;%s, rank&#160;%d out of&#160;%d processors\n",   processor_name, rank, size);
   gethostname(processor_name, 320);
   printf ("locally obtained hostname&#160;%s\n", processor_name);
 }
 MPI_Finalize(); /* Finish the MPI application and release sources*/
</pre>
<p><br />
As you can see, MPI_* routines are used to run the example over multiple nodes (k2hnode1, k2hnode2 in our example).
More details about MPI APIs could be found at <a rel="nofollow" class="external autonumber" href="http://www.open-mpi.org/doc/">[10]</a>
</p>
<h1><span class="mw-headline" id="Demos_included_in_MCSDK-HPC_distribution">Demos included in MCSDK-HPC distribution</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=19" title="Edit section: Demos included in MCSDK-HPC distribution">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<p>HPC provides various Open MPI demos which run on an A15 cluster, as listed in the table below.
</p>
<table style="width: 900px; height: 98px" cellspacing="1" cellpadding="1" width="900" summary="summary" border="1">

<tbody><tr>
<td bgcolor="#c0c0c0"><b>Sample Application</b>
</td>
<td bgcolor="#c0c0c0"><b>Details</b>
</td></tr>
<tr>
<td>testmpi
</td>
<td>Basic MPI loopback test from node A to node B (good to verify A-&gt;B connectivity). Reports host names of connected nodes.
</td></tr>
<tr>
<td>nbody
</td>
<td>Simplified 3D nbody example, up to 1000 particles
</td></tr>
<tr>
<td>Open MPI_examples
</td>
<td>Open MPI examples including connectivity_c, hello_c, ring_c
</td></tr></tbody></table>
<p><br />
MPI paradigm can be combined with OpenCL or OpenMPACC paradigm on the same node. MPI will be used for sharing workload between multiple SoCs (actually A15s), and within same SoC workload item is dispatched (from A15) to DSP. An example employing both paradigms is opencl+Open MPI demo available as a part of MCSDK-HPC. There are several additional examples explaining how to use Open MPI+openmpacc as well ( /usr/share/ti/examples/Open MPI+openmpacc).
<br />
</p>
<h1><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=20" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<ul><li><b>Open MPI</b>:<a rel="nofollow" class="external autonumber" href="http://www.open-mpi.org/">[11]</a></li>
<li><b>Open MPI Training Documents</b>:<a rel="nofollow" class="external free" href="http://www.open-mpi.org/video/">http://www.open-mpi.org/video/</a></li>
<li><b>TI Open MPI User Guide</b>:<a rel="nofollow" class="external text" href="MCSDK_HPC_3-17.html">MPI</a></li>
<li><b>MCSDK-HPC Latest Download</b>: <a rel="nofollow" class="external autonumber" href="http://software-dl.ti.com/sdoemb/sdoemb_public_sw/mcsdk_hpc/latest/index_FDS.html">[12]</a></li>
<li><b>MCSDK-HPC Getting Started Guide</b>: <a rel="nofollow" class="external autonumber" href="MCSDK_HPC_3-3.html">[13]</a></li>
<li><b>MCSDK-HPC Support</b>:<a rel="nofollow" class="external autonumber" href="http://e2e.ti.com/support/applications/high-performance-computing/f/952.aspx">[14]</a></li></ul>
<h1><span class="mw-headline" id="Troubleshooting_Guidelines">Troubleshooting Guidelines</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=21" title="Edit section: Troubleshooting Guidelines">edit</a><span class="mw-editsection-bracket">]</span></span></h1>
<h2><span class="mw-headline" id="Dependencies">Dependencies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=22" title="Edit section: Dependencies">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>This version of Open MPI is dependent on other packages for proper operation. If installed as part of the mcsdk-hpc product, it is likely that these dependencies are already resolved.  However, they are listed here for your knowledge and as a first step in any troubleshooting.
</p>
<ul><li>MPM-transport Module: This package allows the A15 to read/write the shared memory from user space. "ls -l /dev/dsp*" should list read/write permission for usr/group/other. Dynamic share library objects should be installed in /usr/lib ("ls -l /usr/lib/libmpmtransport.*so").</li>
<li>UIO kernel module: This package allows access to certain SoC configuration registers without having "root" privilages. "ls -l /dev/uio*" should list read/write permissions for devices /dev/uio8 and /dev/uio9. "lsmod" should indicate presence of "uio_module_drv"</li>
<li>Hyperlink LLD (low level driver) dynamic shared objects in /usr/lib of target file system ("ls -l libhyplnk.*so")</li>
<li>Password-less SSH communication between master (node which creates all slave processes) and slave nodes. This is typically achieved by copying keys to slave nodes, e.g. using ssh-copy-id SLAVE_NODE_ID</li>
<li>Create list of participating nodes, in /etc/hosts.</li></ul>
<h1><span class="mw-headline" id="FAQ">FAQ</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;section=23" title="Edit section: FAQ">edit</a><span class="mw-editsection-bracket">]</span></span></h1>

<!-- 
NewPP limit report
Cached time: 20201130221538
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.066 seconds
Real time usage: 0.070 seconds
Preprocessor visited node count: 103/1000000
Preprocessor generated node count: 126/1000000
Post‐expand include size: 8/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 0/5000000 bytes
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 -total
-->
</div>
<!-- Saved in parser cache with key procwiki:pcache:idhash:33143-0!canonical and timestamp 20201130221538 and revision id 206745
 -->
<div class='hf-nsfooter' id='hf-nsfooter-'><table style="text-align:center; background:white; width:100%; text-align:left; height: 65px border-top: 1px solid; border-bottom: 1px solid; border-left: 1px solid; border-right: 1px solid; border-width: 1px; border-style: solid;">
<tr>
<td width="305px"><a href="File_E2e.html" class="image"><img alt="E2e.jpg" src="https://processors.wiki.ti.com/images/8/82/E2e.jpg" width="305" height="63" /></a>
</td>
<td>{{
<ol><li>switchcategory:MultiCore=</li></ol>
<ul><li>For technical support on MultiCore devices, please post your questions in the <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/dsp/c6000_multi-core_dsps/default.aspx">C6000 MultiCore Forum</a></li>
<li>For questions related to the BIOS MultiCore SDK (MCSDK), please use the <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/embedded/f/355.aspx">BIOS Forum</a></li></ul>
<p>Please post only comments related to the article <b>MCSDK HPC 3.x OpenMPI</b> here.<i></i>
</p>
</td>
<td>Keystone=
<ul><li>For technical support on MultiCore devices, please post your questions in the <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/dsp/c6000_multi-core_dsps/default.aspx">C6000 MultiCore Forum</a></li>
<li>For questions related to the BIOS MultiCore SDK (MCSDK), please use the <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/embedded/f/355.aspx">BIOS Forum</a></li></ul>
<p>Please post only comments related to the article <b>MCSDK HPC 3.x OpenMPI</b> here.<i></i>
</p>
</td>
<td>C2000=<i>For technical support on the C2000 please post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/microcontrollers/tms320c2000_32-bit_real-time_mcus/f/171.aspx">The C2000 Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td>DaVinci=<i>For technical support on DaVincoplease post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/dsp/davinci_digital_media_processors/default.aspx">The DaVinci Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td>MSP430=<i>For technical support on MSP430 please post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/microcontrollers/msp43016-bit_ultra-low_power_mcus/default.aspx">The MSP430 Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td>OMAP35x=<i>For technical support on OMAP please post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/dsp/omap_applications_processors/default.aspx">The OMAP Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td>OMAPL1=<i>For technical support on OMAP please post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/dsp/omap_applications_processors/default.aspx">The OMAP Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td>MAVRK=<i>For technical support on MAVRK please post your questions on <a rel="nofollow" class="external text" href="http://e2e.ti.com/support/development_tools/mavrk/default.aspx">The MAVRK Toolbox Forum</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
</td>
<td><i>For technical support please post your questions at <a rel="nofollow" class="external text" href="http://e2e.ti.com/">http://e2e.ti.com</a>. Please post only comments about the article <b>MCSDK HPC 3.x OpenMPI</b> here.</i>
<p>}}
</p>
</td></tr></table>
<table style="border-style:solid; border-width:1px; text-align:center; width:100%;">

<tr style="font-size:150%;">
<td rowspan="2"><a href="File_Hyperlink_blue.html" class="image"><img alt="Hyperlink blue.png" src="https://processors.wiki.ti.com/images/9/9f/Hyperlink_blue.png" width="96" height="96" /></a>
</td>
<td><b>Links</b>
</td></tr>
<tr>
<td>
<table style="text-align: left;">
<tr>
<td style="padding-right: 10px; vertical-align: top;">
<p><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/amplifier_and_linear.page">Amplifiers &amp; Linear</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/audio/audio_overview.page">Audio</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/rfif.page">Broadband RF/IF &amp; Digital Radio</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/clocksandtimers/clocks_and_timers.page">Clocks &amp; Timers</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/dataconverters/data_converter.page">Data Converters</a>
</p>
</td>
<td style="padding-right: 10px; vertical-align: top;">
<p><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/mems/mems.page">DLP &amp; MEMS</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/high_reliability.page">High-Reliability</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/interface/interface.page">Interface</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/logic/home_overview.page">Logic</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/powermanagement/power_portal.page">Power Management</a>
</p>
</td>
<td style="padding-right: 10px; vertical-align: top;">
<p><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/dsp/embedded_processor.page">Processors</a>
</p>
<ul><li><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/dsp/arm.page">ARM Processors</a></li>
<li><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/dsp/home.page">Digital Signal Processors (DSP)</a></li>
<li><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/microcontroller/home.page">Microcontrollers (MCU)</a></li>
<li><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/omap-applications-processors/the-omap-experience.page">OMAP Applications Processors</a></li></ul>
</td>
<td style="padding-right: 10px; vertical-align: top;">
<p><a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/switches_and_multiplexers.page">Switches &amp; Multiplexers</a><br/>
<a rel="nofollow" class="external text" href="http://www.ti.com/lsds/ti/analog/temperature_sensor.page">Temperature Sensors &amp; Control ICs</a><br/>
<a rel="nofollow" class="external text" href="http://focus.ti.com/wireless/docs/wirelessoverview.tsp?familyId=2003&amp;sectionId=646&amp;tabId=2735">Wireless Connectivity</a>
</p>
</td></tr></table>
</td></tr></table>
<div id="tiPrivacy"></div>
</div></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;oldid=206745">https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;oldid=206745</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="Special_Categories.html" title="Special:Categories">Category</a>: <ul><li><a href="https://processors.wiki.ti.com/index.php?title=Category:Pages_with_broken_file_links&amp;action=edit&amp;redlink=1" class="new" title="Category:Pages with broken file links (page does not exist)">Pages with broken file links</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-login"><a href="https://processors.wiki.ti.com/index.php?title=Special:UserLogin&amp;returnto=MCSDK+HPC+3.x+OpenMPI" title="You are encouraged to log in; however, it is not mandatory [o]" accesskey="o">Log in</a></li><li id="pt-createaccount"><a href="Special_RequestAccount.html" title="You are encouraged to create an account and log in; however, it is not mandatory">Request account</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="MCSDK_HPC_3-9.html" title="View the content page [c]" accesskey="c">Page</a></span></li><li id="ca-talk" class="new"><span><a href="https://processors.wiki.ti.com/index.php?title=Talk:MCSDK_HPC_3.x_OpenMPI&amp;action=edit&amp;redlink=1" rel="discussion" title="Discussion about the content page (page does not exist) [t]" accesskey="t">Discussion</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="MCSDK_HPC_3-9.html">Read</a></span></li><li id="ca-viewsource" class="collapsible"><span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=edit" title="This page is protected.&#10;You can view its source [e]" accesskey="e">View source</a></span></li><li id="ca-history" class="collapsible"><span><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="https://processors.wiki.ti.com/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Texas Instruments Wiki" title="Search Texas Instruments Wiki [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search the pages for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="Main_Page.html"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage"><a href="Main_Page.html" title="Visit the main page [z]" accesskey="z">Main Page</a></li><li id="n-All-pages"><a href="Special_AllPages.html">All pages</a></li><li id="n-All-categories"><a href="Special_Categories.html">All categories</a></li><li id="n-recentchanges"><a href="Special_RecentChanges.html" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-randompage"><a href="Package_Reflow_Profiles.html" title="Load a random page [x]" accesskey="x">Random page</a></li><li id="n-help"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents" title="The place to find out">Help</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Toolbox</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="Special_WhatLinksHere/MCSDK_HPC_3-7.html" title="A list of all wiki pages that link here [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="Special_RecentChangesLinked/MCSDK_HPC_3-7.html" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-specialpages"><a href="Special_SpecialPages.html" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-print"><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;printable=yes" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li><li id="t-permalink"><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;oldid=206745" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://processors.wiki.ti.com/index.php?title=MCSDK_HPC_3.x_OpenMPI&amp;action=info" title="More information about this page">Page information</a></li>				</ul>
							</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 18 September 2015, at 19:10.</li>
								<li id="footer-info-copyright">Content is available under <a class="external" rel="nofollow" href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike</a> unless otherwise noted.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="Project_Privacy_policy.html" title="Project:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="Project_About.html" title="Project:About">About Texas Instruments Wiki</a></li>
								<li id="footer-places-disclaimer"><a href="Project_General_disclaimer.html" title="Project:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-termsofservice"><a href="Project_Terms_of_Service.html" title="Project:Terms of Service">Terms of Use</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="http://creativecommons.org/licenses/by-sa/3.0/"><img src="https://processors.wiki.ti.com/resources/assets/licenses/cc-by-sa.png" alt="Creative Commons Attribution-ShareAlike" width="88" height="31"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="http://www.mediawiki.org/"><img src="https://processors.wiki.ti.com/resources/assets/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/resources/assets/poweredby_mediawiki_132x47.png 1.5x, /resources/assets/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.066","walltime":"0.070","ppvisitednodes":{"value":103,"limit":1000000},"ppgeneratednodes":{"value":126,"limit":1000000},"postexpandincludesize":{"value":8,"limit":2097152},"templateargumentsize":{"value":0,"limit":2097152},"expansiondepth":{"value":2,"limit":40},"expensivefunctioncount":{"value":0,"limit":100},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":0,"limit":5000000},"timingprofile":["100.00%    0.000      1 -total"]},"cachereport":{"timestamp":"20201130221538","ttl":86400,"transientcontent":false}}});});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":246});});</script>
	</body>

<!-- Mirrored from processors.wiki.ti.com/index.php/MCSDK_HPC_3.x_OpenMPI by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 01 Dec 2020 09:52:52 GMT -->
</html>
